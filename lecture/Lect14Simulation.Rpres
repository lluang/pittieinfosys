Simulation
========================================================
author: IE0015 Information Systems Engineering
date: April 2015


Project feedback
================
type:section

CoNVO
========

-  Context - An organization that has a mission that leads to its interest in the analysis.
-  Need - A decision to be made and a question that needs to be answered to support the decision.
-  Vision - A data presentation that answers the need.
-  Outcome - A decision that could be made based on the data presented in the form identified in the vision.

Data description
=============

-  Source - What data was collected, by whom, and why.
-  Volume - How much data.
-  Variety - For each subject in the data, what kinds of infromation.
-  Velocity - Does data change from time period to time period.
  - Subjects added, subjects lost, or do subjects change over time.
-  Veracity - Truth of the data in representing the population you are interested in.
  - Surveys always have an issue of the responders need to understand the question.
  - Do responders have motivation of being honest?


Methodology and results
==============

-  Methodology - What procedure will you do to perform the analysis?
  -  Make a plan
  -  Goal: reader can evaluate the methodology objectively separate from the results.
-  Results - What was the result of performing the methodology.
  -  The result of carrying out the plan.
  -  Goal: An objective presentation of the analysis.
-  Conclusion - Your interpretation of the analysis as it applies to the Context, Need, Vision, Outcome
  -  Goal: The reader can see how the analysis will lead to a resultion to the decision that was described in the CoNVO
  
Sampling
==========

-  Data usually involves sampling from populations
  - You should know how many responders there are and how they may compare to the entire population, but the results are always percentages/ratios.
  - Statisticians can draw conclusions based on the data results and understanding how the sample was created (e.e.g the weighting variables in NSFG and US Census ACS)


Comparing data
==============

-  We understand data through comparisons with a standard.
-  In statistics, we use the null hypothesis to compare against equality.
-  In data analysis, we must find a comparison group.

Simulation
=============
type:section


Simulation in data analysis
=============================
incremental:true
-  Simulation is the use of sampling through random numbers to obtain a distribution of a dependent variable.
-  e.g. Discrete event simulation, Monte Carlo simulation
-  Useful when we do not have a direct way of determining the value of a dependent variable.

Resampling methods
==============================

-  Each determination of the dependent variable (performance measure) based on random simulation is a *sample*.
-  Results in a *sampling distribution* of the dependent variable.
  - Note: this is *not* the same as a single, "correct" answer.
  - Answers in statistics are given as probability distributions.
    - "There is a 95% probability that the true average is between __ and __"
-  We assume there is a true underlying distribution. The simulation allows us to have an estimate of it.

Advantages of simulation
==============================

-  Fewer assumptions:  we do not require that the underlying population is normally distributed or that the sample size is large.
-  Greater accuracy: Many statistical methods are based on having rough upper bounds or use Taylor series expansions.  Simulations can be replicated based on the desired accuracy.
-  Generality - Resampling methods can be applied to a large class of problems, so do not require developing and implementing methods specific to that problem definition.

Using bootstrap sampling
=======================

-  We have a small enough of observed data but not enough to identify a distribution.
-  *Bootstrap* - resample with replacement from the observed data.

Steps in the bootstrap
=========================
1.  Create samples of the independent variables $x_1^*, ..., x_M^*$, called *resamples*, by sampling with replacement from the data.

2.  Calculate the statistic of interest $S(x_1^*), ..., S(x_M^*)$ for each resample. The distribution of the result of the resample is the *bootstrap distribution*.

3.  The bootstrap distribution gives information about the sampling distribution of the original, unobservable, statistic *S*.  It gives an approximation of the center, spread, and shape of *S*.

Standard errors
=================

-  Errors from bootstrapping come from two sources
  -  The observed data does not the same exact distribution as the population from which it was drawn.
  -  The resampled sample will have a different distribution than the observed data.
  
Standard error example
============================

-  Look at a sample where the underling population is normally distributed $X \sim N(\mu=3, \sigma=1)$.
-  Look at $\bar{x}$
-  We know that $\bar{x} \sim N(\mu, 1/\sqrt{n})$


Example
=====

```{r}
options(digits=2)
srs <- rnorm(25, mean = 3)
resamps <- replicate(1000, sample(srs, 25, TRUE), simplify = FALSE)
xbarstar <- sapply(resamps, mean, simplify = TRUE)
hist(xbarstar, breaks = 40, prob = TRUE, xlim=c(2, 4))
curve(dnorm(x, 3, 0.2), add = TRUE)
```

Compare simulation to the theoretical mean
=========================================

-  Expected confidence interval of the mean
  - $sd/\sqrt{n} = 1/\sqrt{25} = 0.2$
```{r}
mean(xbarstar)
sd(xbarstar)
```
-  Within expectation

Improving the errors
======================
incremental: true

-  What would happen if we took more samples?
-  More samples would make the sample look more like the observed data.
  - Does not make the observed data closer to the true underlying population distribution.
  
Implementing using the boot package
===================

-  The function `boot` is in the *boot* package.
-  Define a summary function, then apply it using bootstrap.

```{r}
library(boot)
mean_fun <- function(x, indices) mean(x[indices])
boot(data = srs, statistic = mean_fun, R = 1000)
```

Try the same thing with a different statistic
========================

```{r}
median_fun <- function(x, indices) median(x[indices])
boot(data = rivers, statistic = median_fun, R = 1000)
```

Confidence intervals
===================

- Typically, we calculated confidence intervals by assuming that outputs were normally distributed.
  - e.g. use c.i. as a function of the *t*-distribution.
- Using simulation, we can apply the definition of the c.i. directly.
  - "95% of the estimates are between __ and __"
  - Using simulation, use the 0.025 and 0.975 quantiles of the simulation results. (e.g. equal tails)
  - Another choice if you believe that the distribution is to use the smallest range that gives a 0.95 interval.
    - Question: Why are these not the same thing?
  
C.I. example
====================

```{r}
btsamps <- replicate(2000, sample(stack.loss, 21, TRUE), simplify = FALSE)
thetast <- sapply(btsamps, median, simplify = TRUE)
mean(thetast)
```

Percentile interval of the median
===============
```{r}
median(stack.loss)
quantile(thetast, c(0.025, 0.975))
```

Now, using the boot function
===============
```{r}
med_fun <- function(x, ind) median(x[ind])
med_boot <- boot(stack.loss, med_fun, R = 2000)
boot.ci(med_boot, type = c("perc", "norm", "bca"))
```

Note: *bca* = bias-corrected and adjusted

Use with the t-interval
=======================

- We usually use the *t*-statistic to calculate confidence intervals.

$$\bar{X} \pm t_{\alpha/2, df=n-1}\frac{S}{\sqrt{n}}$$

-  If we know that our outcome statistics will be bell shaped (e.g. symmetric with a shape similar to the normal), we can use the *t*-statistics along with our bootstrap 

$$statistic  \pm t_{\alpha/2, df=n-1} * SE(statistic)$$


Hypothesis testing
====================

Used to test if two groups are significantly different or if two groups are reasonably similar.  Standard method

- Collect information/data from the two groups and calculate a statistic for comparison.  e.g.
  $$\bar{X}_1 - \bar{X}_2$$
- Presume there is no difference between the groups ($H_0$).  Find the distribution of the statistic in 1.  e.g. assume *mean=0* and calculate *sd*
- Locate the observed value of the statistic with respect to the distribution found in 2. If the value is not in the main body of the distribution, then it provides evidence *against* the null hypothesis.  We usually compute the *p*-value to decide this.

Hypothesis testing with simulation
====================


-  Example: the common dosage for an anti-retroviral drug AZT is 300mg. Higher doses are potentially more effective, but also have more side effects. Question: are the side effects significantly higher?

1.  Calculate the difference in means between two groups.

```{r}
azt300 <- c(284, 279, 289, 292, 287, 295, 285, 279, 306, 298)
azt600 <- c(298, 307, 297, 279, 291, 335, 299, 300, 306, 291)
mean(azt300)
mean(azt600)
delta300600 <- mean(azt600) - mean(azt300)
delta300600
```

Significance with bootstrap
===========================

2.  Resample from the data to generate a distribution of differences.
  -  Presuming there are no differences, sample 10 from the combined data to generate a *a300* group, with the remainder in the *a600* group. (*Q: why 10?*)
  -  Calculate and save the differences in mean.
3.  Draw the *permutation distribution* of these differences. Locate the observed difference (10.9) and determine its *p-value* from the *permutation distribution*. If *p* is small, this is evidence against the null hypothesis

```{r}
set.seed(1234)
permdistfun <- function(azt){
  pdelta <- c()
  tf10 <- c(rep(FALSE, 10), rep(TRUE, 10))
  select <- sample(tf10, 20, replace =FALSE)
  azt3 <- azt[select]
  azt6 <- azt[!select]
  delta <- mean(azt6)-mean(azt3)
  delta
}
```


Now, apply bootstrap and to get the permutation distribution

=====
```{r}
azt <- c(azt300, azt600)
samples = 1000
pdist <- sort(replicate(samples,permdistfun(azt)))
```
```{r, echo=FALSE}
summary(pdist)
hist(pdist, breaks=seq(-20, 20, by=1))
```


Find the p-value of the observed difference
=================
```{r}
qdist <- max(which(pdist<delta300600))/samples
pvalue <- min(qdist, 1-qdist)
pvalue
```

Compare to normal hypothesis testing
==========================

```{r}
t.test(x=azt300, y=azt600)
```

-  Why the difference?

Summary
======

-  Simulation often used when some data is unobservable.
  -  Ask the question: could I have seen the result I saw if *X* was true?
-  Resampling can be used for hypothesis testing in the case of little data or if normal assumption is not true.
  -  Bootstrapping can be used to 




