---
title: "Lecture 12: Linear Regression"
author: "IE0015 Information Systems Engineering"
date: "April 3, 2017"
output:
  html_document: default
  slidy_presentation: default
---

```{r}
options(digits=2)
library(ggplot2, quietly = TRUE)
library(HH, quietly = TRUE)
library(lmtest, quietly = TRUE)
library(GGally, quietly = TRUE)
```



Project Report
=====

- Maximum of 8 pages.
- CoNVO
  -  Vision should be the realized data presentation
  -  Outcome should be a hypothetical outcome based on what you actually saw in your visualization.
- Data discussion: Volume, Velocity, Variety, Varacity
- Process - What did you do (discussion and process map)
- Outcome (Visualization and hypothetical Outcome based on actual results)
- Appendices allowed if needed (does not count against the 8 pages).

Project Presentation
========

- Maximum 8 minutes
- 8-10 slides
- Cover CONVO, data source and 4 V's
  - Vision and Outcome introduced in a pre-project form
- Process map of data manipulation and data processing
- Actual visualizations (realization of the V in CoNVO)
- Hypothetical outcome, given what you actually saw

Linear Regression
=================

Basic philosophy
=========

-  Variables *X* and *Y*
-  *X* is NOT random, but *Y* is.
-  We believe that *Y* depends in *some* way on *X*.
  - $E(Y) = f(x)$
  - $Y = \mu(x) = \beta_o + \beta_1 x_1 + \beta_2 x_2 + \ldots + \epsilon$

Implication
========

-  For each given value of *x*, we observe a random value of *Y* normally distributed with a mean at the regression line.
-  Observations at each value of *x* should be independent, and have the same standard deviations.

Distribution of residuals independent of the independent variable
===========================
```{r, echo=FALSE}
 # open window
plot(c(0,5), c(0,6.5), type = "n", xlab="x", ylab="y")
## the x- and y-axes
abline(h=0, v=0, col = "gray60")
# regression line
abline(a = 2.5, b = 0.5, lwd = 2)
# normal curves
x <- 600:3000/600
y <- dnorm(x, mean = 3, sd = 0.5)
lines(y + 1.0, x)
lines(y + 2.5, x + 0.75)
lines(y + 4.0, x + 1.5)
# pretty it up
abline(v = c(1, 2.5, 4), lty = 2, col = "grey")
segments(1,3, 1+dnorm(0,0,0.5),3, lty = 2, col = "gray")
segments(2.5, 3.75, 2.5+dnorm(0,0,0.5), 3.75, lty = 2, col = "gray")
segments(4,4.5, 4+dnorm(0,0,0.5),4.5, lty = 2, col = "gray")
```


How we use linear regression
===================

- Hypothesis testing
- Simplifying models
- Analysis of designed experiments
- Predictions
- Missing value imputation

How it works
======================

- Assumptions:
  -  That *y* is a linear function of *x*
  -  *Y* is a combination of signal ($\mu(x)$) and random noise ($\epsilon$)
    -  $y_i = \mu(x) + \epsilon_i$
  -  Errors are normally distributed
    - $\epsilon \sim norm(0, \sigma)$

Applying linear regression
==================

- Prepare data (into a data frame)
- Specifying a model
- Evaluating the model to identify the best parameters
  - Optimization to find the parameters that lead to the least squared error
- Model diagnostics  (goodness of fit)

Prepare data and specifying the model
==========

-  In *R*, you can either create a data frame or have the dependent (*Y*) and independent (*x*) variables in vectors of equal length
-  model specification is of the form:
  - *dependent ~ independent1 + independent2*
  - *Petal.Width ~ Petal.Length*
  - Constant is implied. Can be removed by specifying `- 1` in the model
  - Interaction terms can be included using `independent1:independent2`

Evaluating the model
============
- Linear regression is included in R as the *lm* function.

```{r}
data(iris)
irispetal <- lm(Petal.Width ~ Petal.Length, data=iris)
```
```{r, echo=FALSE}
ggplot(iris, aes(x = Petal.Length, y=Petal.Width, color=Species)) + geom_point()
```
Model output
==========
```{r}
irispetal$coefficients
```
```{r, echo=FALSE}
ggplot(iris, aes(x = Petal.Length, y=Petal.Width)) + geom_point() + geom_smooth(method='lm', formula=y~x)
```

Model evaluation
======

-  Evaluate the quality of the linear regression model.
-  Sum of squares error between the prediction and the observed data.
-  The difference between the prediction and the observed data are the *residuals*
-  The estimate for standard deviation of the residuals is the *residual standard error*
```{r}
head(residuals(irispetal))
anova(irispetal)
```

Look at the model results
===========
```{r}
summary(irispetal)
```

Confidence intervals
===========
-  For each coefficient we get an estimate and a standard error.
  - $S=\sqrt{S^2}$, $S^2=\frac{SSE}{n-2}$
-  We can use the *t-statistic* with the standard error to get confidence intervals
-  `confint` give us 95% confidence intervals
```{r}
confint(irispetal)
```

Evaluating the model
==========

-  Consider the total variation in the data.
  - How much is explained by the model?
-  Start with the *total sum of squares (SSTO)*.
$$SSTO = \sum_{i=1}^n \left(Y_i - \bar{Y}\right)^2$$

Breaking up the SSTO
=================

- *SSTO* has two components: regression and error.
- *SSTO = SSR + SSE*
- $SSR = \sum_{i=1}^n \left(\widehat{Y_i} - \bar{Y}\right)^2$
- $SSE = \sum_{i=1}^n \left(Y_i - \widehat{Y_i}\right)^2$
- *Total variation = Explained variation + Unexplained variation*
- Goal: a model that explains more of the variation.

Coefficient of determination
========================

-  $r^2$ is the proportion of the total variation that is explained by the regression model.

$$r^2 = 1 - \frac{SSE}{SSTO}$$

-  Note: $r^2$ can compare models, but what is a *good* value of $r^2$ is context dependent. There may be variation that can never be explained with the available data.

Analysis of Variance
=========================

-  The way to display the partitions of the sum squares is through an *Analysis of Variance* table.
```{r}
anova(irispetal)
```
$$r^2 = 1 - \frac{SSE}{SSTO} = 1-\frac{6.3}{80.3 + 6.3} \approx 0.93$$

Adjusted R-squared
==========================

-  We are biased towards simpler models (fewer independent variables).
$$Adjusted.R^2 = \bar{R}^2 = \left(R^2- \frac{p}{n-1}\right) \left(\frac{n-1}{n-p-1}\right)$$
-  *p* - number of explanatory variables
-  *n* - number of cases in data


Evaluating individual coefficients
============

- The linear regression summary shows an estimate and standard error of each coefficient.
- $t$-test used to test if each coefficient is significant.

```{r, echo=FALSE}
summary(irispetal)
```

Evaluating predictions
===========
- Question: how good is our estimate of $\mu(x)$ for any given value of $x$?
- It should be based on the confidence interval of the mean of $\mu(x)$ and the standard deviation of $Y$ at $x$.
- Usually use 95\% C.I.

Confidence and prediction intervals in R
==============

```{r}
new = data.frame(Petal.Length=c(2,4, 6))
predict(irispetal, newdata=new, interval="confidence")
predict(irispetal, newdata=new, interval="prediction")
```

Confidence and prediction intervals
========
```{r}
ci.plot(irispetal) # from HH library
```

Confidence interval plot
================
- Prediction intervals are much larger than confidence intervals.
- Intervals generally curve out as you reach the extremes

```{r, echo=FALSE}
data(cars)
cars.lm <- lm(dist ~ speed, data = cars)
ci.plot(cars.lm)
```

Analysis of Residuals
==========
- Normality assumption
- Constant variance assumption
- Independence

Normality
================

-  *Normality* - Errors should be normally distributed if a model is good.
-  Shapiro.Wilk test
```{r}
shapiro.test(residuals(irispetal))
```

Density plot
=========
-  Note: regression models are generally robust to violations of normality, so more important is that the residuals are not too skewed.
```{r}
densityplot(irispetal$residuals)
```

QQ-plot
=======
-  Quantile-Quantile plot (QQ-plot) compares the sorted residuals against what they would be if they were normally distributed.

```{r}
qqnorm(irispetal$residuals)
qqline(irispetal$residuals)
```

Constant variance assumption
===========

-  Breusch-Pagan test
```{r}
library(lmtest)
bptest(irispetal)
```
-  Reject constant variance test.
-  Note: this assumption is vary often violated.


Plot residuals to check variance
=============
-  Plot fitted values against standardized residuals.
-  If constant variance, should be flat.
```{r}
plot(irispetal, which=3)
```

Independence assumption
=================

- Test for auto-correlation using Durbin-Watson test.
```{r}
dwtest(irispetal, alternative="two.sided")
```

Independence plot
=================

-  Look for any pattern or structure.
-  Points should be scattered evenly.
```{r}
plot(irispetal, which=1)
```

Remedies
========

-  Change the model.
  -  Is something missing?
  -  Should something be removed?
  -  Should a variable be transformed?

Some causes for violating the regression assumptions
==============

-  Mean response is not linear
  - Try polynomial or other non-linear model
  - Try a transform (e.g. log)
-  Error variance not constant or distribution not normal
  - Box-Cox transformations
-  Error distribution not independent
  - Use auto-regressive time-series models


Special observations
==================

-  Outliers
-  Influential observations


Influential observations
======================

-  Some observations play a large role in the model results.
-  This is a problem, we do not like our procedures to be influenced by the value of a single observation.
-  If it does, that observation deserves extra attention.
  - What happened?

Measuring the influence of an observations
=========================

-  Measure the change in the statistic if the observation was removed.
  - DFBETAS
  - Suspicious if it is greater than $2/\sqrt{n}$ (for large data sets) or 1 (for small data sets)

```{r}
dfb <- dfbetas(irispetal)
head(dfb)
```

Check the influence of an observation on itself
==========================

-  Check the influence of an observation on its own fitted value.
  - DFFITS
  - Flag observations where $|dffit|>1$
```{r}
dff <- dffits(irispetal)
head(dff)
```

Cook's distance
================

-  Measure of the influence of each observation against all of the others.
$$D_i = \frac{E^2_i}{(p+1)S^2} * \frac{h_{ii}}{(1-h_{ii})^2}$$
-  Compares the leverage ($h_{ii}$, the measure of being an outlier in a regression model) against the residual.
-  Compare to quantiles of a *f(df1=2, df2=n-2)* distribution.
  - Greater than 50th percentile is extreme.
-  Plot Residuals vs Leverage
  - Look for points in isolation or above the Cook's distance.
```{r}
cooksD <- cooks.distance(irispetal)
F0.50 <- qf(0.5, df1=2, df2 = length(irispetal)-2)
cooksD[which(cooksD > F0.50)]  
```


Overview of diagnostics plots
===========
incremental:true
```{r, echo=FALSE}
par(mfrow=c(2,2))
plot(irispetal)
par(mfrow=c(1,1))
```

- Why does this model fail (look at the constant variance assumption)

Iris data
===========

- Maybe we should add another term to the linear regression

```{r, echo=FALSE}
ggplot(iris, aes(x = Petal.Length, y=Petal.Width, color=Species)) + geom_point()
```


Add the species
==============


```{r}
irispetal2 <- lm(Petal.Width ~ Petal.Length + Species, data=iris)
par(mfrow=c(2,2))
plot(irispetal2)
par(mfrow=c(1,1))
```
Multiple linear regression
===================

$$Y=X\beta + \epsilon$$

where
$$\epsilon \sim mvnorm(mean=0, sigma=\sigma^2 I_{nXn})$$

Look at the relationships between all variables
====================
```{r}
ggpairs(data = iris)
```

Try a model with all relationships
=====================

```{r}
iris.lm <- lm(Petal.Width ~ Petal.Length + Sepal.Width + Sepal.Length + Species, data = iris)
iris.lm
```

Results
======
```{r, echo=FALSE}
summary(iris.lm)
```

Analysis of variance
==============
```{r, echo=FALSE}
anova(iris.lm)
```

Sum of squares of residuals
========

$$MSE = S^2 = \frac{SSE}{n-(p-1)}$$

- And residual standard error is

$$S = \sqrt{S^2}$$


Confidence and prediction intervals
====================

-  We can get confidence and prediction intervals the same way
```{r, echo=FALSE}
new <- data.frame(Petal.Length = c(3.5, 4, 4.5), Sepal.Width=c(2.5, 3.0, 3.5),
                  Sepal.Length=c(4.5,5, 5.5), Species=c("versicolor", "versicolor", "versicolor"))
```
```{r}
predict(iris.lm, newdata=new, interval="confidence")
predict(iris.lm, newdata=new, interval="prediction")
```

Coefficient of determination
==================

-  In multiple linear regression, adding explanatory variables normally increases $R^2$.
-  It is more important to use adjusted $R^2$ which includes a penalty for additional terms.

$$Adjusted.R^2 = \bar{R}^2 = \left(R^2- \frac{p}{n-1}\right) \left(\frac{n-1}{n-p-1}\right)$$

F-test
===========

- The F-test is also modified for the multiple linear regression.
$$F=\frac{SSR/p}{SSE/(n-p-1)}$$
```{r}
summary(iris.lm)$fstatistic
```

t-test for individual explanatory variables
====================

```{r, echo=FALSE}
summary(iris.lm)
```

Add in an interaction term
=======================

-  Two independent variables may in fact be related, so try to capture the dependence between variables.
-  Implemented in R using the `:`, e.g. *X1:X2*
```{r}
iris.lm.inter <- lm(Petal.Width ~ Petal.Length + Species + Petal.Length:Species, data=iris)
```

Results
======
```{r}
summary(iris.lm.inter)
```

And we can see how it shows up in confidence intervals
=====

```{r}
confint(iris.lm.inter)
```


Summary
========

-  Linear regression
-  Uses of liner regression
-  Implementation
-  Checking assumptions
-  Model evaluation
-  Multiple linear regression
