---
Title: "Lecture 13 Simulation"
author: "IE0015 Information Systems Engineering"
date: "April 2017"
output:
  html_document: default
  slidy_presentation: default
---

```{r}
library(ggplot2)
library(foreach)
library(desolve)
```

Simulation
=============


Simulation in data analysis
=============================

-  Simulation is the use of sampling through random numbers to obtain a distribution of a dependent variable.
-  e.g. Discrete event simulation (IE 1083), Monte Carlo simulation
-  Useful when we do not have a direct way of determining the value of a dependent variable.

Some types of computer simulation
================

-  Monte Carlo
-  Bootstrap (resampling)
-  Discrete event simulation
-  Continuous simulation
-  Agent based simulation

Monte Carlo
=====================

Monte Carlo Simulation
==============

-  Based on repeated use of random numbers.
  -  Name refers to the casinos in Monte Carlo by France
-  If the problem can be stated as the interaction of independent random numbers
- Often used for integration type problems. (area under the curve or probability)


Random numbers in R
===================



Example of Monte Carlo
=================

- Generate 1000 x,y pairs where 
  - $x \sim U(0,1)$
  - $y \sim U(0,1)$
- Test if this is true
 
$$x^2 + y^2 <=1$$
In R
========

```{r}
maxi = 1000
i = (1:maxi)
x <- runif(maxi)
y <- runif(maxi)
incircle <- ifelse(x^2 + y^2 <= 1, 1.0, 0.0)
ratio <- unlist(foreach(i=1:maxi) %do% mean(incircle[1:i]))
```

Monte Carlo
============

```{r}
head(ratio)
```

- What is this doing?


Monte Carlo convergence
======================

-  Run Monte Carlo until the confidence interval of the estimate of the mean is within a certain tolerance.
-  Take the first 30 iterations (Monte Carlo runs are fast)
-  Compute the sample standard deviation $s$.
-  Solve for $n$

$$CI = z_{\alpha/2} s/\sqrt{n}$$
```{r}
mean(ratio)
```

Bootstrap
======

Resampling methods
==============================

-  Each determination of the dependent variable (performance measure) based on random simulation is a *sample*.
-  Results in a *sampling distribution* of the dependent variable.
  - Note: this is *not* the same as a single, "correct" answer.
  - Answers in statistics are given as probability distributions.
    - "There is a 95% probability that the true average is between __ and __"
-  We assume there is a true underlying distribution. The simulation allows us to have an estimate of it.

Advantages of simulation
==============================

-  Fewer assumptions:  we do not require that the underlying population is normally distributed or that the sample size is large.
-  Greater accuracy: Many statistical methods are based on having rough upper bounds or use Taylor series expansions.  Simulations can be replicated based on the desired accuracy.
-  Generality - Resampling methods can be applied to a large class of problems, so do not require developing and implementing methods specific to that problem definition.
-  What if? - We can use samples to create a study population with certain characteristics (e.g. oversample currently underrepresented sub-population) and explore what would happen if the current population changes.


Using bootstrap sampling
=======================

-  We have a small enough of observed data but not enough to identify a distribution.
-  *Bootstrap* - resample with replacement from the observed data.

Steps in the bootstrap
=========================
1.  Create samples of the independent variables $x_1^*, ..., x_M^*$, called *resamples*, by sampling *with replacement* from the data.

2.  Calculate the statistic of interest $S(x_1^*), ..., S(x_M^*)$ for each resample. The distribution of the result of the resample is the *bootstrap distribution*.

3.  The bootstrap distribution gives information about the sampling distribution of the original, unobservable, statistic *S*.  It gives an approximation of the center, spread, and shape of *S*.

Standard errors
=================

-  Errors from bootstrapping come from two sources
  -  The observed data does not the same exact distribution as the population from which it was drawn.
  -  The resampled sample will have a different distribution than the observed data.
  
Standard error example
============================

-  Look at a sample where the underling population is normally distributed $X \sim N(\mu=3, \sigma=1)$.
-  Look at $\bar{x}$
-  We know that $\bar{x} \sim N(\mu, 1/\sqrt{n})$
-  Take mean of 25 samples $N(3,1)$ (sd = $1/\sqrt{25}$)
-  1000 samples

Example
=====


```{r, echo=FALSE}
options(digits=2)
srs <- rnorm(25, mean = 3)
resamps <- replicate(1000, sample(srs, 25, TRUE), simplify = FALSE)
xbarstar <- sapply(resamps, mean, simplify = TRUE)
hist(xbarstar, breaks = 40, prob = TRUE, xlim=c(2, 4),
  main="Mean of 25 N(3,1) distributions, 1000 replications",
  xlab="Mean of replication")
curve(dnorm(x, 3, 0.2), add = TRUE)
```

Compare simulation to the theoretical mean
=========================================

-  Expected confidence interval of the mean
  - $sd/\sqrt{n} = 1/\sqrt{25} = 0.2$
```{r}
mean(xbarstar)
sd(xbarstar)
```
-  Within expectation

Improving the errors
======================

-  What would happen if we took more samples?
-  More samples would make the sample look more like the observed data.
  - Does not make the observed data closer to the true underlying population distribution.
  
Implementing using the boot package
===================

-  The function `boot` is in the *boot* package.
-  Define a summary function, then apply it using bootstrap.

```{r}
library(boot)
mean_fun <- function(x, indices) mean(x[indices])
boot(data = srs, statistic = mean_fun, R = 1000)
```

Try the same thing with a different statistic
========================

```{r}
median_fun <- function(x, indices) median(x[indices])
boot(data = rivers, statistic = median_fun, R = 1000)
```

Confidence intervals
===================

- Typically, we calculated confidence intervals by assuming that outputs were normally distributed.
  - e.g. use c.i. as a function of the *t*-distribution.
- Using simulation, we can apply the definition of the c.i. directly.
  - "95% of the estimates are between __ and __"
  - Using simulation, use the 0.025 and 0.975 quantiles of the simulation results. (e.g. equal tails)
  - Another choice if you believe that the distribution is to use the smallest range that gives a 0.95 interval.
    - Question: Why are these not the same thing?
  
C.I. example
====================

```{r}
btsamps <- replicate(2000, sample(stack.loss, 21, TRUE), simplify = FALSE)
thetast <- sapply(btsamps, median, simplify = TRUE)
mean(thetast)
```

Percentile interval of the median
===============
```{r}
median(stack.loss)
quantile(thetast, c(0.025, 0.975))
```

Now, using the boot function
===============
```{r}
med_fun <- function(x, ind) median(x[ind])
med_boot <- boot(stack.loss, med_fun, R = 2000)
boot.ci(med_boot, type = c("perc", "norm", "bca"))
```

Note: *bca* = bias-corrected and adjusted

Use with the t-interval
=======================

- We usually use the *t*-statistic to calculate confidence intervals.

$$\bar{X} \pm t_{\alpha/2, df=n-1}\frac{S}{\sqrt{n}}$$

-  If we know that our outcome statistics will be bell shaped (e.g. symmetric with a shape similar to the normal), we can use the *t*-statistics along with our bootstrap 

$$statistic  \pm t_{\alpha/2, df=n-1} * SE(statistic)$$


Hypothesis testing
====================

Used to test if two groups are significantly different or if two groups are reasonably similar.  Standard method

- Collect information/data from the two groups and calculate a statistic for comparison.  e.g.
  $$\bar{X}_1 - \bar{X}_2$$
- Presume there is no difference between the groups ($H_0$).  Find the distribution of the statistic in 1.  e.g. assume *mean=0* and calculate *sd*
- Locate the observed value of the statistic with respect to the distribution found in 2. If the value is not in the main body of the distribution, then it provides evidence *against* the null hypothesis.  We usually compute the *p*-value to decide this.

Hypothesis testing with simulation
====================


-  Example: the common dosage for an anti-retroviral drug AZT is 300mg. Higher doses are potentially more effective, but also have more side effects. Question: are the side effects significantly higher?

1.  Calculate the difference in means between two groups.

```{r}
azt300 <- c(284, 279, 289, 292, 287, 295, 285, 279, 306, 298)
azt600 <- c(298, 307, 297, 279, 291, 335, 299, 300, 306, 291)
mean(azt300)
mean(azt600)
delta300600 <- mean(azt600) - mean(azt300)
delta300600
```

Significance with bootstrap
===========================

2.  Resample from the data to generate a distribution of differences.
  -  Presuming there are no differences, sample 10 from the combined data to generate a *a300* group, with the remainder in the *a600* group. (*Q: why 10?*)
  -  Calculate and save the differences in mean.
3.  Draw the *permutation distribution* of these differences. Locate the observed difference (10.9) and determine its *p-value* from the *permutation distribution*. If *p* is small, this is evidence against the null hypothesis

```{r}
set.seed(1234)
permdistfun <- function(azt){
  pdelta <- c()
  tf10 <- c(rep(FALSE, 10), rep(TRUE, 10))
  select <- sample(tf10, 20, replace =FALSE)
  azt3 <- azt[select]
  azt6 <- azt[!select]
  delta <- mean(azt6)-mean(azt3)
  delta
}
```


Now, apply bootstrap and to get the permutation distribution

=====
```{r}
azt <- c(azt300, azt600)
samples = 1000
pdist <- sort(replicate(samples,permdistfun(azt)))
```
```{r, echo=FALSE}
summary(pdist)
hist(pdist, breaks=seq(-20, 20, by=1))
```


Find the p-value of the observed difference
=================
```{r}
qdist <- max(which(pdist<delta300600))/samples
pvalue <- min(qdist, 1-qdist)
pvalue
```

Compare to normal hypothesis testing
==========================

```{r}
t.test(x=azt300, y=azt600)
```

-  Why the difference?

Other types of simulation
========

Continuous simulation
=================

- AKA system dynamics
- Change in system state described by differential equations
- Simulate by advancing state of system in time steps
$$\frac{dx}{dt} = x (\alpha - \beta y)$$
$$\frac{dy}{dt} = -y (\gamma - \delta x)$$
- e.g. predator prey dynamics
  - Prey population based on birthrate and amount of predators
  - Predator population based on eating prey


```{r}
library(deSolve)
 
LotVmod <- function (Time, State, Pars) {
    with(as.list(c(State, Pars)), {
        dx = x*(alpha - beta*y)
        dy = -y*(gamma - delta*x)
        return(list(c(dx, dy)))
    })
}
 
Pars <- c(alpha = 2, beta = .5, gamma = .2, delta = .6)
State <- c(x = 10, y = 10)
Time <- seq(0, 100, by = 1)
out <- as.data.frame(ode(func = LotVmod, y = State, parms = Pars, times = Time))
```

Predator-prey example
===========
```{r, echo=FALSE}
predprey <- ggplot(out, aes(x = time)) + geom_line(aes(y=x, color="Foxes")) + geom_line(aes(y=y, color="Rabbits")) + scale_color_manual("", breaks=c("Foxes", "Rabbits"), values = c("Red", "Blue")) + ylab("Population") + xlab("Time") + ggtitle("Foxes and Rabbit Interactions")
predprey 
```

Discrete event simulation
=============

- Create entities that interact with each other.
- Simulation advanced based on when the next interaction will be (discrete time step)
- In R, use the `simmer` package.
- This will be covered IE 1083
  - Models tend to be easier to build using a package like Arena or Simio.
  - Programming language based simulation more flexible in analysis.

Agent based simulation
==================

- Entities in system (agents) have information.
- At each interaction, the agents involved use the information they have to make a decision on what to do next.
- E.g. disease modeling. If two people interact, check to see if one is already infected, then determine if the infection spreads.

Summary
======

-  Simulation often used when some data is unobservable.
  -  Ask the question: could I have seen the result I saw if *X* was true?
-  Resampling can be used for hypothesis testing in the case of little data or if normal assumption is not true.
  -  Bootstrapping can be used to identify what the system would look like if more data were available or if the source population changes.




